{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxmwKicddCy6JgVuCMNPNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sona-gj/Sentiment-Analysis-of-Social-Media-Data/blob/final/SMA_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coed works better in Jupiter Notebook"
      ],
      "metadata": {
        "id": "6pYF37-DQIyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "fDZ18ltyQajC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import contractions\n",
        "import gensim.downloader as api\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_selection import chi2,SelectKBest\n",
        "from sklearn.svm import LinearSVC\n",
        "from pylab import barh, plot, yticks, show, grid, xlabel, figure\n",
        "from sklearn.feature_selection import chi2\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "LA5Yv8nCOuuS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords if not already done\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "XH76SThKQX4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chi2_selector = SelectKBest(chi2, k=100)"
      ],
      "metadata": {
        "id": "9cjfXI0HO2lk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "ZfcVu9d9QVTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model = api.load('glove-wiki-gigaword-100')"
      ],
      "metadata": {
        "id": "a3AqzVmxO5qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\", trainable=False)"
      ],
      "metadata": {
        "id": "qIlxlYOOO7N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the dataset csv file\n",
        "\n",
        "data = pd.read_csv(\"dataset.csv\")\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "-vHZHcZeO8wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing unwanted columns\n",
        "\n",
        "data = data.drop(columns = [\"id\",\"username\",\"created_at\",\"user followers count\",\"replycount\",\"retweetcount\",\"likecount\",\"quotecount\",'media', 'retweetedTweet', 'quotedtweet',\n",
        "       'inReplyToTweetId', 'inReplyToUser', 'mentionedUsers','hashtags','language'])\n",
        "data.head(3)\n"
      ],
      "metadata": {
        "id": "U9HDe4VvO_ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Cleaning\n",
        "\n",
        "def remove_unwanted(text):\n",
        "\n",
        "    #expand the words\n",
        "    text = contractions.fix(text)\n",
        "\n",
        "    #convert the text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #remove the urls\n",
        "    text = re.sub(r'https?:\\/\\/\\S*', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove mentions, special characters, and additional URLs\n",
        "    text = ' '.join(re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
        "    return text\n",
        "\n",
        "#before cleaning\n",
        "tweet_2582_before_cleaning = data.loc[2582, 'text']\n",
        "print(tweet_2582_before_cleaning)\n",
        "\n",
        "#clean the text\n",
        "data['cleaned_text'] = data['text'].apply(remove_unwanted)\n",
        "\n",
        "#after cleaning\n",
        "tweet_2582_cleaned = data.loc[2582, 'cleaned_text']\n",
        "print(tweet_2582_cleaned)\n",
        "\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "nNOqj0SMPAkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic Preprocessing\n",
        "\n",
        "# Set stopwords & custom words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "custom_words = {'stockmarketcrash','stockmarkets','bearmarket','stock','market'}\n",
        "\n",
        "#Stemming - PorterStemmer, Lemmatizer - WordNet\n",
        "ps = PorterStemmer()\n",
        "# wnl = WordNetLemmatizer()\n",
        "\n",
        "# Function to remove stopwords and custom words\n",
        "def remove_unwanted(text):\n",
        "    words = word_tokenize(text)\n",
        "    wordsFiltered = [w for w in words if w not in stopWords and w not in custom_words]\n",
        "    stemmed_words = [ps.stem(word) for word in wordsFiltered]\n",
        "    return ' '.join(stemmed_words)\n",
        "    # lemmatized_words = [wnl.lemmatize(word) for word in wordsFiltered]\n",
        "    # return ' '.join(lemmatized_words)\n",
        "\n",
        "data['cleaned_text'] = data['cleaned_text'].apply(remove_unwanted)\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "9ze8W0AdPDzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the dataset into trainset and testset in the ratio- 80:20\n",
        "\n",
        "x = data.drop(columns = ['text_sentiment'])\n",
        "y = data.text_sentiment\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)\n",
        "\n",
        "#printing shapes of testing and training sets :\n",
        "\n",
        "print(\"shape of original dataset :\", data.shape)\n",
        "print(\"shape of input - training set\", x_train.shape)\n",
        "print(\"shape of output - training set\", y_train.shape)\n",
        "print(\"shape of input - testing set\", x_test.shape)\n",
        "print(\"shape of output - testing set\", y_test.shape)"
      ],
      "metadata": {
        "id": "HTcv3YvKPFVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#representing the trainset and testset pictorically\n",
        "\n",
        "labels = ['Training Set', 'Testing Set']\n",
        "sizes = [ x_train.shape[0], x_test.shape[0]]\n",
        "\n",
        "plt.bar(labels, sizes, color=['skyblue', 'lightgreen'])\n",
        "plt.title('Dataset Split: Training vs Testing')\n",
        "plt.ylabel('Total Samples')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rq7i72T2PGyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate performance\n",
        "\n",
        "def evaluate_performance(y_test,y_pred, embedding, classifier):\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred, average='weighted')\n",
        "  recall = recall_score(y_test, y_pred, average='weighted')\n",
        "  f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "  print(f\"Accuracy for the {classifier} using {embedding}: {accuracy:.4f}\")\n",
        "  print(f\"Precision for the {classifier} using {embedding}: {precision:.4f}\")\n",
        "  print(f\"Recall for the {classifier} using {embedding}: {recall:.4f}\")\n",
        "  print(f\"F1-Score for the {classifier} using {embedding}: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "V_fDVFaOPHg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier\n",
        "\n",
        "def svm_classifier(X_train_model,X_test_model,y_train,y_test,embedding):\n",
        "  clf = LinearSVC(max_iter=10000)\n",
        "  clf.fit(X_train_model, y_train)\n",
        "  y_pred = clf.predict(X_test_model)\n",
        "  evaluate_performance(y_test,y_pred,embedding,'SVM')"
      ],
      "metadata": {
        "id": "2oVdvRalPJK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression classifier\n",
        "\n",
        "def logistic_regression_classifier(X_train_model,X_test_model,y_train,y_test,embedding):\n",
        "  classifier = LogisticRegression(max_iter=1000)\n",
        "  classifier.fit(X_train_model, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = classifier.predict(X_test_model)\n",
        "  evaluate_performance(y_test,y_pred,embedding,'Logistic Regression')"
      ],
      "metadata": {
        "id": "ybC6pFQ5PK6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot chi-sqaure\n",
        "\n",
        "def plot_chi_sqaure(chi2score,embedding):\n",
        "  print(\"Chi2 for \"+ embedding)\n",
        "  # Assuming vectorizer and chi2score are already computed\n",
        "  wscores = list(zip(vectorizer.get_feature_names_out(), chi2score))  # list to allow sorting multiple times\n",
        "  wchi2 = sorted(wscores, key=lambda x: x[1])  # Sort by chi-square score\n",
        "\n",
        "  # Extract the top 25 features based on chi-square scores\n",
        "  topchi2 = list(zip(*wchi2[-25:]))  # Unzips the sorted list\n",
        "\n",
        "  # Extract labels and scores\n",
        "  labels = topchi2[0]  # Feature names\n",
        "  scores = topchi2[1]  # Chi-square scores\n",
        "\n",
        "  # Plotting\n",
        "  x = np.arange(len(scores))  # x-axis is just the indices\n",
        "\n",
        "  figure(figsize=(6, 6))\n",
        "  barh(x, scores, align='center', alpha=.2, color='g')  # Horizontal bar chart\n",
        "  plot(scores, x, '-o', markersize=2, alpha=.8, color='g')  # Plot line with markers\n",
        "  yticks(x, labels)  # Add labels to the y-ticks\n",
        "  xlabel('$\\\\chi^2$')  # X-axis label with escaped backslash\n",
        "    # X-axis label\n",
        "  grid(True)  # Add grid for better readability\n",
        "  show()  # Show the plot\n"
      ],
      "metadata": {
        "id": "qpyfhQCzPNnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chi-sqaure\n",
        "\n",
        "def chi_sqaure(X_train_chi, X_test_chi, y_train, y_test,embedding):\n",
        "\n",
        "  # computing chi2 for each feature\n",
        "  chi2score = chi2(X_train_chi,y_train)[0]\n",
        "  plot_chi_sqaure(chi2score,embedding)\n",
        "  print(\"Chi2 scores for \"+ embedding+\": \", chi2score)\n",
        "\n",
        "  # Apply chi-square feature selection to the training data\n",
        "  X_train_selected = chi2_selector.fit_transform(X_train_chi, y_train)\n",
        "\n",
        "  # Apply the same feature selection to the test data\n",
        "  X_test_selected = chi2_selector.transform(X_test_chi)\n",
        "\n",
        "  #training\n",
        "  svm_classifier(X_train_selected,X_test_selected,y_train,y_test,embedding)\n",
        "  logistic_regression_classifier(X_train_selected,X_test_selected,y_train,y_test,embedding)"
      ],
      "metadata": {
        "id": "ZevV7Y5RPOe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bag of words\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(x_train['cleaned_text'])\n",
        "X_train_array = X_train.toarray()\n",
        "\n",
        "X_test = vectorizer.transform(x_test['cleaned_text'])\n",
        "X_test_array = X_test.toarray()\n",
        "\n",
        "chi_sqaure(X_train_array, X_test_array, y_train, y_test,'BagOfWords')\n",
        "\n",
        "print(X_train_array.shape)\n",
        "print(X_test_array.shape)"
      ],
      "metadata": {
        "id": "M5TYVaaePQe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tf_idf = vectorizer.fit_transform(x_train['cleaned_text'])\n",
        "X_test_tf_idf = vectorizer.transform(x_test['cleaned_text'])\n",
        "\n",
        "chi_sqaure(X_train_tf_idf, X_test_tf_idf, y_train, y_test,'TF-IDF')\n",
        "\n",
        "print(X_train_tf_idf.shape)\n",
        "print(X_test_tf_idf.shape)"
      ],
      "metadata": {
        "id": "wW3qwNclPSdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to average word vectors for a document\n",
        "\n",
        "def document_to_vector(text, model, num_features):\n",
        "    words = word_tokenize(text)\n",
        "    feature_vector = np.zeros((num_features,), dtype='float32')\n",
        "    n_words = 0\n",
        "\n",
        "    # Only include words that are in the Word2Vec vocabulary\n",
        "    for word in words:\n",
        "        if word in model.key_to_index:\n",
        "            n_words += 1\n",
        "            feature_vector = np.add(feature_vector, model[word])\n",
        "\n",
        "    if n_words > 0:\n",
        "        feature_vector = np.divide(feature_vector, n_words)\n",
        "\n",
        "    return feature_vector"
      ],
      "metadata": {
        "id": "Ijlku738PURT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#word2vec training\n",
        "\n",
        "\n",
        "num_features = word2vec_model.vector_size\n",
        "\n",
        "\n",
        "X_train_word2vec = np.array([document_to_vector(text, word2vec_model, num_features) for text in x_train['cleaned_text']])\n",
        "X_test_word2vec = np.array([document_to_vector(text, word2vec_model, num_features) for text in x_test['cleaned_text']])\n",
        "\n",
        "\n",
        "#training\n",
        "svm_classifier(X_train_word2vec,X_test_word2vec,y_train,y_test,'Word2Vec')\n",
        "logistic_regression_classifier(X_train_word2vec,X_test_word2vec,y_train,y_test,'Word2Vec')\n",
        "\n",
        "# Check the shape of the resulting feature matrices\n",
        "print(X_train_word2vec.shape)\n",
        "print(X_test_word2vec.shape)\n"
      ],
      "metadata": {
        "id": "0ceUgnHEPWOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# glove training\n",
        "\n",
        "\n",
        "num_features = glove_model.vector_size\n",
        "\n",
        "\n",
        "X_train_glove = np.array([document_to_vector(text, glove_model, num_features) for text in x_train['cleaned_text']])\n",
        "X_test_glove = np.array([document_to_vector(text, glove_model, num_features) for text in x_test['cleaned_text']])\n",
        "\n",
        "#training\n",
        "svm_classifier(X_train_glove,X_test_glove,y_train,y_test,'GloVe')\n",
        "logistic_regression_classifier(X_train_glove,X_test_glove,y_train,y_test,'GloVe')\n",
        "\n",
        "# Check the shape of the resulting feature matrices\n",
        "print(X_train_glove.shape)\n",
        "print(X_test_glove.shape)"
      ],
      "metadata": {
        "id": "M0_wx1bEPXuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#USE\n",
        "\n",
        "\n",
        "X_train_use = encoder(tf.constant(x_train['cleaned_text'].to_list()))  # Convert pandas dataframe to list and then to tf.constant\n",
        "X_test_use = encoder(tf.constant(x_test['cleaned_text'].to_list()))\n",
        "\n",
        "\n",
        "X_train_use = X_train_use.numpy()\n",
        "X_test_use = X_test_use.numpy()\n",
        "\n",
        "#training\n",
        "svm_classifier(X_train_use,X_test_use,y_train,y_test,'USE')\n",
        "logistic_regression_classifier(X_train_use,X_test_use,y_train,y_test,'USE')\n",
        "\n",
        "print(X_train_use.shape)\n",
        "print(X_test_use.shape)\n"
      ],
      "metadata": {
        "id": "QneMCQ1fPZfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}